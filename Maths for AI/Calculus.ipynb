{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f87007-e1cc-41a2-9d2b-a306d5b38187",
   "metadata": {},
   "source": [
    "## Calculus for AI & ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6968d-d87c-4fb4-ac64-dd04b274da23",
   "metadata": {},
   "source": [
    "### 1. What is Calculus? (Why it matters in AI)\n",
    "Calculus helps us understand:\n",
    "- How outputs change when inputs change\n",
    "- How to minimize loss/error in ML models\n",
    "\n",
    "**In ML:**\n",
    "- Training = minimizing a loss function\n",
    "- Minimization uses derivatives (gradients)\n",
    "\n",
    "**Placement takeaway:**\n",
    "> Calculus = backbone of **optimization**, **gradient descent**, **backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abfaec-b605-4b20-bd87-2426ce59ae78",
   "metadata": {},
   "source": [
    "### 2. Functions (Core Foundation)\n",
    "\n",
    "**Definition**\n",
    "\n",
    "A **function** maps input → output  \n",
    "\\[\n",
    "f(x) = y\n",
    "\\]\n",
    "\n",
    "**In ML:**\n",
    "- Model = function  \n",
    "- Input = features  \n",
    "- Output = prediction  \n",
    "\n",
    "Example:\n",
    "```text\n",
    "y = f(x) = wx + b\n",
    "```\n",
    "\n",
    "**Must know:**\n",
    "- Domain, range\n",
    "- Linear vs non-linear functions\n",
    "\n",
    "**Interview line:**\n",
    "> \"A machine learning model is essentially a function that maps input features to predictions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91310845-cf54-46b2-962d-948b3fd92669",
   "metadata": {},
   "source": [
    "### 3. Composite Functions (Function inside Function)\n",
    "**Definition**\n",
    "\n",
    "$(f \\circ g)(x) = f(g(x))$\n",
    "\n",
    "**In Neural Networks:**\n",
    "- Each layer applies a function\n",
    "- Output of one layer → input of next\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "z = wx + b\n",
    "a = σ(z)\n",
    "```\n",
    "\n",
    "**Why important:**\n",
    "- Backpropagation uses chain rule\n",
    "- Deep learning = nested composite functions\n",
    "\n",
    "**Interview focus:**\n",
    "> \"Neural networks are compositions of functions optimized using chain rule.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07ba20-1d86-4f8b-9dad-da3d559f16dd",
   "metadata": {},
   "source": [
    "### 4. Scalar Multiplication & Addition (Functions)\n",
    "**Scalar Multiplication**\n",
    "\n",
    "$g(x) = c \\cdot f(x)$\n",
    "\n",
    "**Addition**\n",
    "\n",
    "$h(x) = f(x) + g(x)$\n",
    "\n",
    "**In ML:**\n",
    "- Scaling loss functions\n",
    "- Combining multiple loss terms (regularization)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Loss = MSE + λ * Regularization\n",
    "```\n",
    "\n",
    "**Must know:**\n",
    "- Scaling affects learning speed, not direction\n",
    "-Addition combines objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af0f69-1e41-45b1-98f1-8ed18afb3aa0",
   "metadata": {},
   "source": [
    "### 5. Scalar Multiplication & Addition (Inputs)\n",
    "**Definition**\n",
    "Transforming input before function:\n",
    "\n",
    "$f(ax + b)$\n",
    "\n",
    "**In ML:**\n",
    "- Feature scaling\n",
    "- Bias terms\n",
    "- Normalization\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "x_normalized = (x - mean) / std\n",
    "```\n",
    "\n",
    "**Practical importance:**\n",
    "- Faster convergence\n",
    "- Stable gradients\n",
    "\n",
    "**Interview point:**\n",
    "> \"Feature scaling improves gradient descent convergence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc8f78-6af3-4ae4-8d55-78fe29578e9a",
   "metadata": {},
   "source": [
    "### 6. Differentiation (Heart of ML)\n",
    "**Definition**\n",
    "Derivative = rate of change\n",
    "\n",
    "$\\frac{dy}{dx}$\n",
    "\n",
    "**Interpretation:**\n",
    "- Slope of curve\n",
    "- Direction to reduce loss\n",
    "\n",
    "**In ML:**\n",
    "- Gradient = derivative of loss wrt parameters\n",
    "- Used in Gradient Descent\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Loss(w) → dLoss/dw\n",
    "```\n",
    "\n",
    "**Absolute must-know:**\n",
    "- What derivative represents\n",
    "- Why derivative = optimization tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a941c8-c7d8-41ba-b691-85c8fe771715",
   "metadata": {},
   "source": [
    "### 7. Differentiation Rules (Placement MUST)\n",
    "**Power Rule**\n",
    "\n",
    "$\\frac{d}{dx}(x^n) = nx^{n-1}$\n",
    "\n",
    "**Constant Rule**\n",
    "\n",
    "$\\frac{d}{dx}(c) = 0$\n",
    "\n",
    "**Sum Rule**\n",
    "\n",
    "$\\frac{d}{dx}(f + g) = f' + g'$\n",
    "\n",
    "**Chain Rule (MOST IMPORTANT)**\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$\n",
    "\n",
    "**ML connection:**\n",
    "- Backpropagation = repeated chain rule\n",
    "\n",
    "**Interview warning:**\n",
    "- If you don't understand chain rule → DL will feel magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab14c4-91d3-4ebb-b089-83b2288f3ddf",
   "metadata": {},
   "source": [
    "### 8. Finding Minima / Maxima (Optimization)\n",
    "**Concept**\n",
    "- Minimum = lowest loss\n",
    "- Maximum = rarely used in ML\n",
    "\n",
    "**Method:**\n",
    "- Find derivative\n",
    "- Set derivative = 0\n",
    "- Check nature of point\n",
    "\n",
    "$\\frac{dL}{dw} = 0$\n",
    "\n",
    "**In ML:**\n",
    "- Training = find minimum loss\n",
    "- Gradient descent approximates this\n",
    "\n",
    "**Practical reality:**\n",
    "- We don't solve analytically\n",
    "- We use iterative updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41086b5-ac9a-47c2-8df9-045418674e58",
   "metadata": {},
   "source": [
    "### 9. Gradient Descent (Practical Implementation)\n",
    "**Update Rule**\n",
    "\n",
    "$w = w - \\alpha \\cdot \\frac{dL}{dw}$\n",
    "\n",
    "**Where:**\n",
    "- α = learning rate\n",
    "\n",
    "**Intuition:**\n",
    "- Move in direction of steepest decrease\n",
    "\n",
    "**What you actually use in code:**\n",
    "```python\n",
    "w = w - lr * grad\n",
    "```\n",
    "\n",
    "**Interview note:**\n",
    "- Learning rate too high → divergence\n",
    "- Too low → slow convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
