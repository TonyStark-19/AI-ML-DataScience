{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0406278a-c31a-4df9-bc61-b5673fe140b8",
   "metadata": {},
   "source": [
    "## Linear Regression — Complete Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad5850-7619-43f3-8efe-8fd4d5a4d2ae",
   "metadata": {},
   "source": [
    "### 1. What is Linear Regression?\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to predict a continuous numerical value by modeling the linear relationship between input features (X) and target (y).\n",
    "\n",
    "> Goal: Find the best-fit straight line that minimizes prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b73729-42f0-4dbd-a3cb-5149eeeeb0e9",
   "metadata": {},
   "source": [
    "### 2. Types of Linear Regression\n",
    "**Simple Linear Regression**\n",
    "- One independent variable\n",
    "- Equation:\n",
    "\\[\n",
    "y = mx + b\n",
    "\\]\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "- Multiple independent variables\n",
    "$Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7cfe6-9052-49a9-96c9-1270fd81f6cb",
   "metadata": {},
   "source": [
    "### 3. Key Terminology\n",
    "- Independent Variable (X) → Input / Feature\n",
    "- Dependent Variable (y) → Output / Target\n",
    "- Coefficient (β) → Impact of X on y\n",
    "- Intercept (β₀) → Value of y when X = 0\n",
    "- Residual → Actual − Predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44183c82-55cd-46bf-b674-47fc08b801ac",
   "metadata": {},
   "source": [
    "### 4. How Linear Regression Works (Intuition)\n",
    "- Draw a line that best represents the trend in data\n",
    "- Minimize the distance between actual and predicted points\n",
    "- Uses least squares method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf6080-9951-4d82-b867-7342eea01653",
   "metadata": {},
   "source": [
    "### 5. Cost Function (Loss Function)\n",
    "Mean Squared Error (MSE)\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2$\n",
    "\n",
    "**Purpose:**\n",
    "- Penalizes large errors\n",
    "- Differentiable → easy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca0f6b-d66e-4632-8421-5410e4fc4f44",
   "metadata": {},
   "source": [
    "### 6. Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ce27b-4d09-4e30-9b95-135fde3dcbe9",
   "metadata": {},
   "source": [
    "`1. Normal Equation`\n",
    "\n",
    "$\\mathbf{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "\n",
    "- Fast for small datasets\n",
    "- Computationally expensive for large data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36809f3-bdbb-4374-8e4f-762b955e26b6",
   "metadata": {},
   "source": [
    "`2. Gradient Descent`\n",
    "\n",
    "$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J(\\theta_{old})$\n",
    "\n",
    "- Iterative optimization\n",
    "- Works well for large datasets\n",
    "- Requires learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e907e6-439e-4d48-8d07-3a712f47250c",
   "metadata": {},
   "source": [
    "### 7. Assumptions of Linear Regression (Very Important)\n",
    "- Linearity → Linear relation between X and y\n",
    "- Independence → Observations independent\n",
    "- Homoscedasticity → Constant variance of errors\n",
    "- Normality of errors → Residuals are normally distributed\n",
    "- No multicollinearity → Features not highly correlated\n",
    "\n",
    "> Violating assumptions → unreliable predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f86c00-32bb-4747-b8fa-dc49b92e1721",
   "metadata": {},
   "source": [
    "### 8. EDA Before Linear Regression\n",
    "**EDA helps check:**\n",
    "- Linearity (scatter plots)\n",
    "- Outliers (boxplots)\n",
    "- Multicollinearity (correlation heatmap)\n",
    "- Skewness (distribution plots)\n",
    "- Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7270fc1-9a92-474e-aa67-c10d362c5b72",
   "metadata": {},
   "source": [
    "### 9 Feature Scaling (When Needed)\n",
    "- Required for gradient descent\n",
    "- Not required for normal equation\n",
    "\n",
    "**Methods:**\n",
    "- Standardization\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904f203-9321-46a1-bce7-31e5f887133c",
   "metadata": {},
   "source": [
    "### 10. Handling Categorical Variables\n",
    "- One-Hot Encoding\n",
    "- Label Encoding (rare)\n",
    "\n",
    "> Avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa7661-d84a-4bcd-b807-756011e25ed1",
   "metadata": {},
   "source": [
    "### 11. Model Evaluation Metrics\n",
    "- **MAE**: Mean Absolute Error  \n",
    "- **MSE**: Mean Squared Error  \n",
    "- **RMSE**: Root Mean Squared Error  \n",
    "- **R²**: Variance explained  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c6c6e-af57-4a16-b7c6-fb74158e721b",
   "metadata": {},
   "source": [
    "### 12. Overfitting & Underfitting\n",
    "- Underfitting → Too simple model\n",
    "- Overfitting → Too complex model\n",
    "\n",
    "**Solution:**\n",
    "- Add/remove features\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac2b2a-652b-4665-afa7-6faa7bead148",
   "metadata": {},
   "source": [
    "### 13. Regularization (Important)\n",
    "**Ridge Regression (L2)**\n",
    "- Penalizes large coefficients\n",
    "- Reduces overfitting\n",
    "\n",
    "**Lasso Regression (L1)**\n",
    "- Can set coefficients to zero\n",
    "- Performs feature selection\n",
    "\n",
    "**Elastic Net**\n",
    "- Combination of L1 + L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e7abd-6349-4a68-b26f-a1eae8e78c2c",
   "metadata": {},
   "source": [
    "### 14. Interpreting Coefficients\n",
    "- Positive β → y increases as X increases\n",
    "- Negative β → y decreases as X increases\n",
    "- Larger |β| → stronger impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653205c-3916-45de-9332-3a466c9c52fb",
   "metadata": {},
   "source": [
    "### 15. When NOT to Use Linear Regression\n",
    "- Non-linear relationships\n",
    "- High multicollinearity\n",
    "- Complex interactions\n",
    "- Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d39a9a-899b-458b-9fdf-b3b160d9090b",
   "metadata": {},
   "source": [
    "### 16. Common Mistakes\n",
    "- Ignoring assumptions\n",
    "- Not checking residuals\n",
    "- Using LR for classification\n",
    "- Forgetting feature scaling\n",
    "- Blindly trusting R²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ea8ae-c198-430e-9d83-03991be05388",
   "metadata": {},
   "source": [
    "### 17. Interview Questions (Must-Know)\n",
    "- What is linear regression?\n",
    "- What are its assumptions?\n",
    "- Difference between MAE & MSE?\n",
    "- What is multicollinearity?\n",
    "- Ridge vs Lasso?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
